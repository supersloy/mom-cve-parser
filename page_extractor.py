import os
import requests
from bs4 import BeautifulSoup
import re
import globals


def fetch_web_page(url: str, file_name: str, save_dir: str) -> BeautifulSoup:
    try:
        path = f"./{globals.PAGES_DIRECTORY}/{save_dir}"
        if not os.path.exists(path):
            os.makedirs(path)
        filepath = f"{path}/{file_name}.html"
        if os.path.isfile(filepath):
            with open(filepath, "r", encoding="utf-8") as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
        else:
            response = requests.get(
                url, headers=globals.REQUEST_HEADERS, allow_redirects=True)
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(response.text)
            soup = BeautifulSoup(response.text, 'html.parser')

        return soup

    except Exception as e:
        print(e)
        return BeautifulSoup("", 'html.parser')


def find_product_id(mom_name: str, print_links: bool = False) -> int:
    with open("exceptions.txt", "r") as f:
        exceptions = f.read().splitlines()
        exceptions_dict: dict[str, int] = {exception.split(' ')[0]: int(
            exception.split(' ')[1]) for exception in exceptions}
        for exception_key in exceptions_dict:
            if exception_key.lower() in mom_name.lower():
                return exceptions_dict[exception_key]

    search_url: str = f"https://www.google.com/search?q=site:cvedetails.com+{mom_name}"
    soup = fetch_web_page(search_url, mom_name, "GOOGLE_SEARCH_RESULTS")
    try:
        for link in soup.find_all('a', href=True):
            href: str = link['href']
            if print_links:
                print(href)
            if "product" in href:
                match = re.search(r'product/(\d+)', href)
                if match:
                    return int(match.group(1))
            if "version-list" in href:
                match = re.search(r'product/(\d+)/(\d+)/1', href)
                if match:
                    return int(match.group(2))
            if "product_id" in href:
                match = re.search(r'product_id-(\d+)', href)
                if match:
                    return int(match.group(1))
    except Exception as e:
        print(e)
        return -1

    print(f"!Product ID of {mom_name} for CVEDetails not found.")
    print(f"!Please, add the ID to urlExceptions.txt manually.")
    return -1


def find_urls(mom_name: str) -> dict[str, str]:
    versions_base_url = "https://www.cvedetails.com/version-list/1"
    vulnerabilities_base_url = "https://www.cvedetails.com/vulnerability-list"
    product_id = find_product_id(mom_name)
    return {
        "versions": f"{versions_base_url}/{product_id}/",
        "vulnerabilities": f"{vulnerabilities_base_url}//product_id-{product_id}/"
    }


def download_pages(print_info=False):
    for mom_name in globals.TEST_MOM_NAMES:
        if print_info:
            print(f"{mom_name}:")
        for section, url in find_urls(mom_name).items():
            if print_info:
                print(f"    {section: <20}{url}")
            web_page = fetch_web_page(url, section, f"{mom_name}")
            sublinks = web_page.select(
                '[title="List of security vulnerabilities, CVEs"]')
            if (section == "vulnerabilities" and len(sublinks) > 0):
                if print_info:
                    print("    vulnerabilities contain additional web pages:")
                # Fetch the additional pages
                for i in range(1, len(sublinks)):
                    sublink = sublinks[i]
                    fetch_web_page(
                        f"https://www.cvedetails.com{sublink['href']}", f"{section}{i+1}", f"{mom_name}")
                    if print_info:
                        print(f"    {sublink['href']}")


def find_product_ids(print_info=False):
    for mom_name in globals.TEST_MOM_NAMES:
        product_id = find_product_id(mom_name)
        if print_info:
            print(f"Product ID for {mom_name}: {product_id}")


if __name__ == "__main__":
    # Example of module usage:
    download_pages(True)
